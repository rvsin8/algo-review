Chapter 4 - Sorting and Searching 

- Why is sorting worth so much attention ? There are several reasons:
	> sorting is the basic building block that many other algorithms are built around. By 	understanding sorting, we obtain an amazing amount of power to solve other 			problems. 
	> most of the interesting ideas used in the design of algorithms appear in the 			context of sorting, such as divide-and-conquer, data structure and randomized 		algorithms. 
	> computers have historically spent more time sorting than doing anything else. A 		quarter of all mainframe cycles were spent sorting data. Sorting remains the most 		ubiquitous combinatorial algorithm problem in practice. 
	> sorting is the most thoroughly studied problem in computer science. Literally 		dozens of different algorithms are known, most of which possess some particular 		advantage over all the other algorithms in certain situations. 

4.1 Applications of Sorting
- We will review several sorting algorithms and their complexities over the course of this chapter. But the punch-line is this: clever sorting sorting algorithms exist that run in O(n log n). This is a big improvement over naive O(n^2) sorting algorithms for larger values of n. 
- Many important problems can be reduced to sorting, so we can use our clever O(n log n) algorithms to do work that might otherwise seem to require a quadratic algorithm. An important algorithm design technique is to use sorting as a basic building block, because many other problems become easy once a set of items is sorted. 
- Consider the following applications:
	> searching: binary search tests whether an item is in a dictionary in O(log n) time, 		provided the keys are all sorted. Search preprocessing is perhaps the single most 		important application of sorting. 
	> closest pair - given a set of n numbers, how do you find the pair of number that 		have the smallest difference between them? Once the numbers are sorted, the 		closest pair of numbers must lie next to each other somewhere in sorted order. 		Thus a linear time scan through them completes the job, for a O(n log n) time 			including the sorting.
	> element uniqueness - are there any duplicates in a given set of n items ? This is a 	special case of the closest-pair problem above, where we ask if there is a pair 			separated by a gap of zero. The most efficient algorithm sorts the numbers and 		then does a linear scan though checking all adjacent pairs.
	> frequency distribution - given a set of n items, which element overs the largest 		humber of times in the set?  If the items are sorted, we can sweep from left to right 		and count them, since all identical items will be lumped together during sorting. To 		find out how often an arbitrary element k occurs, look up k using binary search in a 		sorted array of keys. By walking to the left of this point until the first element if not k 	and then going the same to the right, we can count this in O(log n + c) time, where 		c is the number of occurrences of k. Even better, the. Umber of instances of k can 		be found in O(log n) time by using binary search to look for the positions of both k - 	e and k + e (where e is arbitrarily small) and then taking the difference of these 		positions. 
	> selection - what is kth largest item in an array ? If the keys are placed in sorted 		order, the 5th largest can be found in constant time by simply looking at the 6th 		position of the array. In particular, the median element appears in the (n/2)nd 			position in sorted order. 
	> convex hulls - what is the polygon of the smallest area that contains a given set 		of n points in two dimensions? The convex hull is like a rubber band stretched over 		the points in the plane and then released. It compresses to just cover the points. 		The convex hull gives a nice representation of the shape of the points and is an 		important building block for more sophisticated geometric algorithms. But how can 		use sorting to construct the convex hull? Once you have the points sorted by x-		coordinate, the points can be inserted from left to right into the hull. Since the right-	most point is always on the boundary, we know that it will appear in the hull. Adding 	this new right-most may cause others top be deleted, but we can quickly identify 		these points because they lie inside the [polygon formed by adding the new point. 		These points will be neighbors of the previous point we inserted, so they will be 		easy to find and delete. The total time is linear after the sorting has been done. 
- While a few of these problems namely median and selection can be solved in linear time using more sophisticated algorithms, sorting provides quick and easy solutions to all of these problems. It is a rare application where the running time of sorting proves to be the bottleneck, especially a bottleneck that could otherwise been removed more clever algorithmics. Never be afraid to spend time sorting, provided you use an efficient sorting routine. 
- Sorting lies at the heart of many algorithms. Sorting the data is one of the first things any algorithm designer should try in the quest for efficiency. 
- Given an efficient algorithm to determine whether two sets (of size m and n, respectively) are disjoint. Analyze the worst-case complexity in terms of m and n, considering the case where m is substantially smaller than n. At least three algorithms comes to mind, all of which are variants of sorting and searching: 
	> first sort the big set - the big set can be sorted in O(n log n) time. We can now do 	a binary search with each of the m elements in the second, looking to see if it exists 	in the big set. The total time will be O((n+m) log n). 
	> first sort the small set - the small set can be sorted in O(m log m) time. We can 		now do a binary search with each of the element of the n elements in the big sets to 	see if it exists in the small one. The total time will be O((n+m) log m).
	> sort both sets - observe that once the two sets are sorted, we no longer have to 		do binary search to detect a common element. We can compare the smallest 			elements of the two sorted sets, and discard the smaller one if they are not 			identical. By repeating this idea recursively on the now smaller sets, we can test for 	duplication in linear time after sorting. The total cost is O(n log n + m log m + n + 		m).
	> so which of these is the fastest method ? Sorting the small set is the best of these 	options.  
- Note that expected linear time can be achieved by hashing, Build a hash table containing the elements of both sets, and verify that collisions in the same bucket are in fact identical elements. In practice, this may be the best solution.

4.2 Pragmatics of Sorting 
- We have seen many algorithmic applications of sorting and we will see several efficient sorting algorithms. One issue stands between them: in what order do we want our items sorted ? The answer to this basic question are application-specific. Consider the following considerations:
	> increasing, or decreasing order ? - a set of keys S are sorted in ascending order 		when Si <= Si+1 for all 1 <= i < n. They are descending order when Si >= Si + 1 for 		all 1 <= i < n. Different applications call for different orders. 
	> strong just the key or an entire record? - sorting a data set involves maintaining 		the integrity of complex data records. A mailing list of names, addresses and phone 	numbers may be sorted by names as they key field, but it has better retain the 		linkage between names and addresses. Thus, we need to specify which field is the 		key field in any complex record, and understand the full extent of each record. 
	> what should we do with equal keys? Elements with equal key values will all bunch 		together in any total order, but sometimes the relative order among the keys 			matters. You may need secondary keys such as an article size, to resolve ties in a 		meaningful way. Sometimes it is required to leave the items in the same relative 		order as in the original permutation. Sorting algorithms that automatically enforce 		this requirement are called stable. Unfortunately few fast algorithms are stable. 		Stability can be achieved for any sorting algorithm by auditing the initial position as 	a secondary key. Of course we would make no decision about equal key order and 		let the ties fall where they may. But beware, certain efficient sort algorithms can run 	into quadratic performance trouble unless explicitly engineered to deal with large 		numbers of ties. 
	> what about non-numerical data ? Alphabetizing is the sorting text strings. 			Libraries have very complete and complicated rules concerning the relative 			collating sequence of characters and punctuations. 
- The right way to specify such matters to your sorting algorithms is with an application-specific pairwise-element comparison function.
- By abstracting the pairwise ordering decision to such a comparison function, we can implement sorting algorithms independently of such criteria. We simply pass the comparison function in as an argument to the sort procedure. Any reasonable programming language has a built-in sort routine as a library function. You are almost always better off using this than writing your own routine. For example, the standard library for c contains the qsort function for sorting: [code].
- Read this page - 108 - to get an understanding of quicskort. 

4.3 Heapsort: Fast Sorting via Data Structures 
- Sorting is a natural laboratory for studying algorithm design paradigms, since many useful techniques lead to interesting sorting algorithms.
- The alert reader should ask why we review the standard sorting when you are better off not implementing them and using built-in library functions instead. The answer is that the design techniques are very important for other algorithmic problems you are likely to encounter. We start with a ds design, because one of the most dramatic algorithmic improvements via appropriate data structures occurs in sorting. Selection sort is a simple-to-code algorithm that repeatedly extracts the smallest remaining element from the unsorted part of the set: [code]. 
- There we partitioned the input array into sorted and unsorted regions. To find the smallest item, we performed a linear sweep through the unsorted portion of the array. The smallest item is then swapped with the its item in the array before moving on to the next iteration. Selection sort performs n iterations, where the average iteration takes n/2 steps, for a total of O(n^2) time. 
- But what if we improve the data structure ? It takes O(1) time to remove a particular item from an unsorted array once it has been located, but O(n) time to find the smallest item. These are exactly the operations supported by priority queues. So what happens if we replace the data structure with a better priority queue implementation, either a heap or a balanced binary tree? Operations within the loop now take O(log n) time each, instead of O(n). Using such a priority queue the loop now take O(log n) time each, instead of O(n). Using such a priority queues implementation speeds up selection sort from O(n^2) to O(n log n).
- The name typically given to this algorithms, heapsort, obscures the relationship between them, but heapsort is nothing but an implementation of selection sort using the right data structure. 

4.3.1 Heaps
- Heaps are simple and elegant data structure for efficiently supporting the priority queues operations insert and extract-min. They work by maintaining a partial order on the set of elements which is weaker than the sorted order (so it can be efficient to maintain) yet stronger than random order (so the minimum element can be quickly identified).
- Power in any hierarchically-structured organization is reflected by a tree, where each node in the tree represents a person, and edge (x,y) implies that x directly supervises (or dominates) y. The fellow at the root sits at the “top of the heat”. In this spirit, a heap-labeled tree is defined to be a binary tree such that the key labeling of each node dominates the key labeling of each of its children. In a min-heap, a node dominates its children by containing a smaller key than they do while in a max-heap parent nodes dominate by being bigger. 
- The most natural implementation of this binary tree would store each key in a node with pointers to its two children. As with binary search trees, the memory used by the pointers can easily outweigh the size of the keys, and use the position of the keys to implicitly satisfy the role of the pointers. 
- We will store the root of the tree in the first position of the array and its left and right children in the second and third positions, respectively. In general, we will store the 2^l-1 keys to the lth level of a complete binary tree from left to right in positions 2^l-1 to 2^l - 1.
- What is nice about this representation is that the positions of the parent and children of the key at position k are readily determined. The left child of k sits in positions 2k and the right child in 2k + 1, while the parent of k holds court in position [k/2]. This we can move around the tree without any pointers. [code]. 
- So, we can store any binary tree in an array without pointers. What is the catch ? Suppose our height h tree was sparse, meaning that the number of nodes n < 2^h. All missing internal nodes still take up space in our structure, since we must represent an n-key tree using exactly n elements of the array. If we did not enforce these structural constraints, we might need an array of size 2^n to store the same elements. Since all but the last level is always filled, the height h of the n element heap is logarithmic bc [formula].
- This implicit representation of binary trees saves memory, but is less flexible than using pointers. We cannot store arbitrary tree topologies without wasting large amounts of space. We cannot move subtrees around by just changing a single pointer, only by explicitly moving each of the elements in the subtree. This loss in flexibility explains why we cannot use this idea to represent binary search trees, but it works just fine for heaps. 
- How can we efficiently search for a particular key in a heap ? We can’t. Binary search does not work because a heap is not a binary search tree. We know almost nothing about the relative order of the n/2 leaf elements in a heap - certainly nothing that lets us avoid doing linear search through them. 

4.3.2 - Constructing Heaps 
    - Heaps can be constructed incrementally, by inserting each new element into the leftmost open spot in the array, namely the (n + 1)st position of a previous n-element heap. This ensures the desired balanced shape of the heap-labeled tree but does not necessarily maintain the dominance ordering of the keys. The key might be less than its parent in a min-heap, or greater than its parent in a max-heap. The solution is to swap any such dissatisfied element with its parent. The parent is now happy, because it is properly dominated. The other child of the parent is still happy, because it now dominated by an element even more extreme than its previous parent. The new element is even happier, but may still dominate its new parent. We now recur at a higher level, bubbling up the new key to proper-position in the hierarchy. Since we replace the root of subtree by a large one at each step, we preserve the heap order elsewhere. 
    - This swap process takes constant time at each level. Since the height of an n-element heap is [lg n], each insertion tales at most O(log n) time. Thus an initial heap of n elements can be constructed in O(n log n) time through n such insertions: [code].

4.3.3 Extracting the Minimum
- The remaining priority queue operations are identifying and deleting the dominant element. Identification is easy, since the top of the heap sits in the first position of the array. Removing the top element leaves a hole in the array. This can be filled by moving the element from the right-most leaf (sitting in the nth position of the array) into the first position. 
- The shape of the tree has been restored but (as after insertion) the labeling of the root may no longer satisfy the heap property. Indeed, this new root may be dominated by both of its children. The root of this min-heap should be the smallest of the three elements, namely the current node and its two children. If the current root is dominant, the heap order has been restored. If not, dominant child should be swapped with the root and the problem pushed down to the next level. 
- This dissatisfied element bubbles down the heap until it dominates all its children, perhaps by becoming a leaf node and ceasing to have any. This percolate-down operation is also called heapify, because it merges two heaps (the subtrees below the original root) with a new key. [code].
- We will reach a left after [lg n] bubble_down steps, each constant time. Thus root deletion is completed in O(log n) time. Exchanging the maximum element with the last element and calling heap repeatedly gives an O(n log n) sorting algorithm, named Heapsort. 
- It runs in worst-case O(n log n) time, which is the best that can be expected from any sorting algorithm. It is an in-place sort, meaning it uses no extra memory over the array containing the elements to be sorted. Although other algorithms prove slightly faster in practice, you won’t go wrong using heap sort for sorting data that sits in the computer’s main memory. Priority aueues are useful data structures. 

4.3.4 Faster Heap Construction (*)
- As we gave seen, a heap can be constructed on n elements by incremental insertion in O(n log n) time. Surprisingly, heaps can be constructed even faster by using our bubble_down procedure and some clever analysis. 
- Suppose we pack the n keys destined for our heap into the first n elements of our priority-queue array. The shape of our heap will be right, but the dominance order will be all messed up. How can we restore it?  Consider the array in reverse order, starting from the last (nth) position. It represents a leaf of the tree and so dominates its nonexistent children. The same is the case for the last n/2 positions in the array because all are leaves. If we continue to walk backwards through the array we will finally encounter an internal node with children. This element may not dominate its children, buts it children represent well-formed (if small) heaps. 
- This is exactly the situation the bubble_down procedure was designed to handle, restoring the heap order of arbitrary root element sitting on top of two sub-heaps. This we can create a heap by performing n/2 non-trivial calls to the bubble_down procedure: [code]. 
- Multiplying the number of calls to bubble_down (n) times an upper bound on the cost of each operation (O(log n)) gives us a running rime analysis of (O(n log n)). This would make it no faster than the incremental insertion algorithm described above. 
- But note that it is indeed upper bound, because only the last insertion will actually take [lg n] steps. Recall that bubble_down takes time proportional to the height of the heaps it is merging. Most of these heaps are extremely small. In full binary tree on n nodes, there are n/2 nodes that are leaves aka height 0, n/4 nodes are height 1, n/8 nodes that are eighth 2 and so on. In general at most [n/2^h+1] nodes of height h, so the cost of building a heap is [formula]. 
- Since this sum is not quite a geometric series, we cant apply the usually identity to get the sum, but rest assured that the puny contribution of the numerator (h) crushed by the denominator (2^h). The series quickly converges to linear. 
- Does it matter that we can construct heaps in linear time instead of O(n log n)? Usually not. The construction time did not dominate the complexity of heap sort, so improving the construction time does not improve its worst-case performance. Still, it is an impressive display of the power of careful analysis, and the free lunch that geometric series convergence can sometimes provide. 
- Given an array-based heap on n elements and a real number x, efficiently determine whether the nth smallest element in the heap is greater than or equal to x. Your algorithm should be O(k) in worst-case, independent of the size of the heap. You do not have to find the nth smallest element; you need only to determine its relationship to x. There are at least two different ideas that lead to correct but inefficient algorithms for this problem: 
	> call extract-min k times, and test whether all of these are less than x. This explicitly sorts the first k elements 	and so gives us more information than the desired answer but it takes O(k log n) time to do so. 
	> the kth smallest element cannot be deeper than the nth level of the heap since the path from it to the root 		must go through elements of decreasing value. Thus we can look at all the elements on the first k levels of the 	heap, and count how many of them are less than x, stopping when we either find k of them or run out of 		elements. This is correct, but takes O(min(n, 2^k)) time, since the top k elements have 2^k elements. 
- An O(k) solution can look at only k elements smaller than x, plus at most O(k) elements greater than x. Consider the following recursively procedure, called at the root with I = 1 with count = k: [code]. If the root of the min-heap is >= x, then no elements in the heap can be less than x, as by definition the root must be the smallest element. This procedure search the children of all nodes of weight smaller than x until either (a) we have found k of them, when it returns 0, or (b) they are exhausted, when it returns a value greater than zero. Thus it will find enough small elements if they exist. 
- But how long does it take ? The only nodes whose children we look at those < x, and at most k of these in total. Each have at most visited two children, so we visit at most 3k nodes, for the total time of O(k), 

4.3.5 Sorting by Incremental Insertion
- Now consider a different approach to sorting via efficient data structures. Select an arbitrary element from the unsorted set and put it in the proper position in the sorted set. [code]. 
- Although insertion sort takes O(n^2) in the worst case, it performs considerably better if the data is almost sorted, since few iterations of the inner loop suffice to sift it into the proper position. 
- Insertion sort is perhaps the simplest example of the incremental insertion technique, where we build up a complicated structure on n items by first building it on n-1 items and then making the necessary changes to add the last item. Incremental insertion proves a particularly useful technique in geometric algorithms. 
- Note that faster sorting algorithms based on incremental insertion follow from more efficient data structures. Insertion into a balanced search tree takes O(log n) per operation or a total of O(n log n) to construct the tree. An in-order traversal reads through the elements in sorted order to complete the job in linear time. 

4.5 Mergesort: Sorting by Divide-and-Conquer 
- Recursive algorithm reduce large problems into smaller ones. A recursive approach to sorting involves partitioning the elements into two groups, sorting each of the smaller problems recursively and then interleaving the two sorted lists to totally order the elements. This algorithm is called mergesort, recognizing the importance of the.interleaving operations: [code].
- The basis case of the recursion occurs when the subarray to be sorted consists of a single element, so no rearrangement is possible, A trace of the execution of merge sort is given in Figure 4.4. Picture the action as it is happenis during an in-order traversal of the top tree, with the array-state transformations reported in the bottom reflected tree.
- The efficiency of merge sort depends upon how efficiently we combine the two sorted halves into a single sorted list. We could concatenate them into one list and call heap sort or some other sorting algorithm to do to, but that would just destroy all the work spend sorting our component lists. Instead we can merge the two lists together. Observe that the smallest overall item in two lists sorted in increasing order as above must sit at the top of one of the two lists. This smallest element can be removed, leaving two sorted lists behind - one slightly shorter than before. The second smallest item overall must be atop one of these lists. Repeating this operation until both lists are empty merges two sorted lists (with a total of n elements between them) into one, using at most n - 1 comparisons or O(n) total work. 
- What is the total running time of mergesort ? It helps to think about how much work is done at each level of the execution tree. If we assume for simplicity that n is a power of two, the kth level consists of all the 2^k calls to merge sort processing subranges of n/2^k elements. Next three paragraphs explain this in a deeper level - read it.
- Mergesort is a great algorithm for sorting linked lists, because it does not rely on random access to elements as does heap sort or quick sort. Its primary diasdavatge is the need for an auxiliary buffer when sorting arrays. It is easy to merge two sorted linked lists without using any extra space by just rearranging the pointers. However, to merge two sorted arrays (or portions of an array), we need use a third array to store the result of the merge to avoid stepping on the component arrays. Consider merging {4,5,6} with {1,2,3}, packed from the left to right in a single array. Without a buffer, we would overwrite the elements of the top half during merging and lose them. 
- Merge sort is a classic divide and conquer algorithm,. We are ahead of the game whenever we can break one large problem into two smaller problems, because the smaller problem is easier to solve. The trick is taking advantage of the two partial solutions to construct a solution of the full problem, as we did with the merge operation.
- [code].

4.6 Quicksort: Sorting by Randomization 
- Suppose we select a random item p from the n items we seek to sort. Quicksort separates the n - 1 other items into two piles: a low pile containing all the elements that appear before the p in sorted order and a high pile containing all the elements that appear after p in sorted order. Low and high denote the array positions we place the respective piles, leaving a single slot between them for p. 
- Such partitioning buys us two things. First, the pivot element p ends up in the exact array position it will reside in the final sorted order. Second, after portioning no element flops to the other side in the final sorted order. Thus we can now sort the elements to the left and right of the pivot independently. This gives us an recursively algorithm, since we can use the partitioning approach to sort each subproblem. The algorithm must be correct since each element ultimately ends up in the proper position: [code]. 
- But how long the entire quicksort take ? They explain this in the next three paragraphs. Read it. 
- Thus, the worst case for quicksort is worse than heapsort or mergesort. To justify its name, quick sort had better be good in the average case. Understanding why requires some intuition about random sampling. 

4.6.1 - Intuition: The Expected Case for Quicksort 
- The expected performance of quicksort depends upon the height of the partition tree constructed by random pivot elements at each step. Mergesort ran in O(n log n) time because we split the keys into two equal halves, sorted them recursively and then merged the halves in linear time. Thus, whenever our pivot element is near the center of the sorted array (the pivot is close to the median element), we get a good split and realize the same performance as merge sort. 
- The rest of this passage goes on about why quick sort is O(nlogn) in the average case, read it. 

4.6.2 - Randomized Algorithms 
- There is an important subtlety about the expected case O(n log n) running time for quicksort. Our quicksort implementation above selected the last element in each sub-array ads the pivot. Suppose this program were given a sorted array as input. If so, at each step it would pick up the worst possible pivot and run in quadratic time. 
- For any deterministic method of pivot selection, there exists a worst-case input instance which will doom us to quadratic time. The analysis presented above made no claim stronger than “quicksort runs in O(n log n) time, with high probability, if you give me randomly ordered data to sort”. But now suppose we add an initial step to our algorithm where we randomly permute the order of the n elements before we try to sort them. Such a permutation can be constructed in O(n) time. This might seem like wasted effort, but it provides the guarantee that we can expect O(n log n) running time whatever the initial input was. The worst case performance still can happen, but it depends only upon how unlucky we are. There is no longer a well-defined “worst case” input. We can now say “randomized quicksort runs in O(n log n) time on any input, with high probability.” 
- Alternatively, we could get the same guarantee by selecting a random element to be the pivot at each step. Randomization is a powerful tool to improve algorithms with bad worst-case but good average-case complexity. It can be used to make algorithms more robust to boundary cases and more efficient on highly structured input instances that confound heuristic decisions (such as sorted input or quicksort). It often lends itself to simple algorithms that provide randomized performance guarantees which are otherwise obtainable only using complicated deterministic algorithms. Proper analysis of randomized algorithms requires some knowledge of probability theory, and is beyond the scope of this book. However, some of the approaches to designing efficient randomized algorithms are readily explainable: 
	> random sampling - want to get an idea of the median value of n things but dont have either the time or space 
	to look at them all? Select a small random sample of the input and study those, for the results should be 		representative. This is the idea behind opinion polling. Biases creep in unless you take a truly random sample, as 	opposed to the first x people you happen to see. To avoid bias, actual polling agencies typically dial random 	phone numbers and hope someone answers. 
	> random hashing - we have claimed that hashing can be used to implement dictionary operation O(1) 	“expected-time”. However, for any hash function there is a given worst-case set of keys that all get hashed to 	the same bucket. But now suppose we randomly select our hash function from a large family of a good ones that 	we did with randomized quicksort. 
	> randomized search - randomization can also be used to drive search techniques such as simulated annealing. 

4.6.3 Is Quicksort Really Quick ?
- There is a clear, asymptotic difference between an O(n log n) algorithm and one that runs in OIn^2). Thus, only the most obstinate reader would doubt my claim that mergesort, heapsort, and quicksort should all outperform insertion sort or selection sort on large enough instances. 
- But how can we compare two O(n log n) algorithms to decide which is faster? How can we prove that quicksort is really quick? Unfortunately, the RAM model and Big Oh analysis provide too coarse a set of tools to make that type of distinction. When faced with algorithms of the same asymptotic complexity, implementation details and system quirks such as cache performance and memory size may well prove to be the decisive factor. 
- - what we can say is that experiments show that when a quicksort is implemented well, it is typically 2-3 faster than mergesort or heapsort. The primary reason is that the operations in the innermost loop are simpler. But I cant argue with you if you don’t believe me when I say quicksort is faster. It is a question whose solution lies outside the analytical tools we are using. The best way to tell us is to implement both algorithms and experiment, 


4.7 Distribution Sort: Sorting via Bucketing
- We could sort names for the telephone book by portioning them according to the first letter of the last name. This will create 26 different piles, or buckets or names. Observe that any name in the J pile must occur after every name in that I pile, but before any name in the K pile. Therefore, we can proceed to sort each pile individually and just concatenate the bunch of sorted piles together at the end. 
- If the names are distributed evenly among the buckets, the resulting 26 sorting problems should each be substantially smaller than the original problem. Further, by now portioning each pile based on the second letter of each name, we generate smaller and smaller piles. The names will be sorted as soon as each bucket contains only a single name. The resulting algorithm is commonly called bucket sort or distribution sort. 
- Bucketing its a very effective idea whenever we are confident that the distribution of data will be roughly uniform. It is the idea that underlies hash tables, kd-trees, and a variety of other practical data structures. The downside of such techniques is that performance can be terrible when the data distribution is not what we expected. Although data structures such as balanced binary trees offer guaranteed worse-case behavior for any input distribution, no such promise exists for heuristic data structures on unexpected input distributions. 
- Sorting can be used to illustrate most algorithm design paradigms. Data structure techniques, divide-and-conquer, randomization and incremental construction all lead to efficient sorting algorithms. 

4.7.1 Lower Bounds for Sorting
- One last issue on the complexity of sorting. We have seen several sorting algorithms that run in worst case O(n log n) time, but none of which are linear. To sort n items certainly requires looking at all of them, sort any sorting algorithm must be (n) in the worst case. Can we close this remaining O(log n) gap ? The answer is now. It explains why - read it. This lower bound is important for several reasons. First, the idea can be extended to give lower bounds for many applications of sorting, including element uniqueness, finding the node and constructing convex hulls. Sorting has one of the nontrivial lower bounds among algorithmic problems. 

4.9 - Binary Search and Related Algorithm 
- Binary search is a fast algorithm for searching in sorted arrays of keys S. To search for key q, we compare q to the middle key S[n/2]. If q appears before S[n/2] it must reside in the top half of S; if not, it must reside in the bottom half of S. By repeating this proves recursively on the correct half, were locate the key in a total of [lg n] comparisons - a big win over the n/2 comparisons expect using sequential search: [code]. This much you probably know. What is important is to have a sense of how fast binary search is. Since standard dictionaries contain 50,000 to 200,000 words we can be certain that the process will terminate within twenty questions. 

4.9.1 Counting Occurrences
- Several interesting algorithms follow from simple variants of binary search. Suppose that we want to count the number of times a given key k occurs in a given sorted array. Because sorting groups all the copies of k into a contiguous block, the problem reduces to finding the right block and then measures its size. The binary search routine presented above anneals us to find the index of an element of the correct block (x) in O(lg n) time. The natural way to identify the boundaries of the block is to sequentially test elements to the left of x until we find the first one that differs from the search key and repeat this search to the right of x. The difference between the indices of the left and the right boundaries plus one gives the count of the number of occurrences of k. Continue reading this until we find out how each search takes O(lg n) time, so we can count the occurrences in the logarithmic time regardless oof the size of the block. 

4.9.2 One Sided Binary Search 
- One sided binary search is Mose useful whenever we are looking for a key that lies close to our current position. 

4.9.3 Square and Other Roots
- The square root of n is the number r such that r^2 = n. Square root computations are performed inside every pocket calculator, but it is instructive to develop an efficient algorithm to compute them. Root-finding algorithms that converge faster than binary search are known for both of these problems. Instead of always testing the midpoint of the interval, these algorithms interpolate to find a test point t closer to the actual root. Still binary search is simple, robust and works as well as possible without additional information on the nature of the function to be computed. Binary search and its variants are quintessential divide-and-conquer algorithms. 

4.10 Divide-and-Conquer 
- One of the most powerful techniques for solving problems is to break them down into smaller, more easily solved pieces. Smaller problems are less overwhelming, and they permit us to focus on details that are lost when we are studying the entire problem. A recursive algorithm starts to become apparent when we can break the problem into smaller instances of the same type os problem. Effective parallel processing requires decomposing jobs into at least as many tasks as processors, and is becoming more important with the advent of cluster computing and multicore processors. 
- Two important algorithm design paradigms are based on breaking down into smaller problems. In Chapter 8, we will see dynamic programming, which typically removes one element from the problem, solves the smaller problem, and then uses the solution to this smaller problem to add back the element in the proper way. Divide-and-conquer instead splits the problem in halves, solves each half, then stitches the pieces back together to form a full solution. 
- To use divide-and-conquer as an algorithm design technique, we must divide the problem into two smaller subproblems, solve each of them recursively and then meld the two partial solutions into one solution to the full problem. Whenever the merging takes less time than solving the two subproblems, we get an efficient algorithm. Mergesort, is the classic example of a divide-and-conquer algorithm. It takes only linear to merge two sorted lists of n/2 elements, each of which was obtained in O(nlgn) time.
- Divide-and-conquer is a design technique with many important algorithms to its credit, including mergesort, the fast Fourier transform, and Strassen’s matrix multiplication algorithm. Beyond binary search and it’s many variants, however, I find it to be difficult design technique to apply in practice. Our ability to analyze divide-and-conquer algorithms rests on our strength to solve the asymptotic of recurrence relations governing the cost of such recursive algorithms. 

4.10.1 Recurrence Relations 
- Many divide-and-conquer algorithms have time complexities that are naturally modeled by recurrence relations. Evaluating such recurrences is important to understandings when divide-and-conquer algorithms perform well and provide an important tool for analysis in general. 
- What is a recurrence relation? It is an equation that is defined in terms of itself. The Fibonacci numbers are described by the recurrence relation Fn = Fn-1 + Fn-2 … 
- Many other natural functions are easily expressed is recurrences. Any polynomial can be represented by recurrence such as the linear function: an = an-1 + 1, a1 = 1 —> an = n
- Any exponential can be represented by a recurrence: [formula].
- Finally, lots of weird functions that cannot be described easily with conventional notation can be represented by a recurrence: [formula].
- This means that recurrence relations are a very versatile way to represent functions. The self-reference property of recurrence relations is shared with recursive programs or algorithms, as the shared roots of both terms reflect. Essentially, recurrence relations provide a way to analyze recursive structures, such as algorithms.

4.10.2 Divide-and-Conquer Recurrences 
- Divide-and-conquer algorithms tend to break a given problem into some number of smaller pieces (say a), each of which is of size n/b. Further, they spend f(n) time to combine these subproblem solutions into a complete result. Let T(n) denote the worst-case time the algorithm takes to solve a problem of size n. Then T(n) is given by the following recurrence relation: [formula].
- Consider the following examples:
	> Sorting - the running time behavior of mergesort is governed by the recurrence T(n) = 2T(n/2) + 	O(n), since the algorithm divides the data into equal-sized halves and then spend the linear time 	merging the halves after they are sorted. In fact, this recurrence evaluates to T(n) = O(n lg n), just 	as we got by our previous analysis. 
	> Binary Search - the running time behavior of binary search is governed by the recurrence T(n) = 	T(n/2) + O(1), since at each step we spend constant time to reduce the problem to an instance 	half its size. In fact, this recurrence evaluates to T(n) = O(lg n), just as we got by our previous 	analysis. 
	> Fast Heap Construction - the bubble.down method of heap construction built an n-element 		heap by constructing two n/2 elements heaps and then merging them with the root in logarithmic 	time. This argument reduces to the recurrence relation T(n) = 2T(n/2) + O(lg n). In fact, this 		recurrence evaluates to T(n) = O(n), just as we got by our previous analysis. 
	> Matrix Multiplication - as discussed in Section 2.5.4, the standard matrix multiplication 		algorithm for two n * n matrices takes O(n^3), because we compute the dot product of n terms for 	each of the n^2 elements in the product matrix. 
- However, Strassen [Str69] discovered, a divide-and-conquer algorithm that manipulates the products of seven n/2 * n/2 matrix products to yield the product of two n * n matrices. This yields a time complexity recurrence T(n) = 7T(n/2) + O(n^2). In fact, this recurrence evaluates to T(n) = O(n^2.81), which seems impossible to predict without solving the recurrence. 

4.10.3 Solving Divide-and-Conquer Recurrences(*) 
- In fact, divide-and-conquer recurrences of the form T(n) = aT(n/b) + f(n) are generally easy to solve, because the solutions typically fall into one of three distinct cases:
	> formula 1
	> formula 2
	> formula 3
- Although this looks somewhat frightening, it really isn’t difficult to apply. The issue is identifying which case of this so0called master theorem holds for your given recurrence. Case 1 holds for heaps construction and matrix multiplication, while Case 2 holds mergseort and binary search. Case 3 generally arises for clumsier algorithm, where the cost of combining the subproblems dominates everything. 
- The three causes of the master theorem correspond to three difference costs which might be dominant as a function of alb and f(n).
	> Case 1: Too many leaves - if the number of leaf nodes outweighs the sum of the 		internal evaluation cost, the total running time is O(n^logba)
	> Case 2: Equal work per level - as we move down the tree, each problem gets 		smaller but there are more of them to solve. If the sum of the internal evaluation 		costs at each level are equal, the total running time is the cost per level(n^logna) 		times the number of levels (Logbn), for a total running time os [expression].
	> Case 3: too expensive a root - if the internal evaluation costs grow rapidly enough 	with n, then the cost of the root evaluation may dominate. if so the total running time 	is O(f(n)).

