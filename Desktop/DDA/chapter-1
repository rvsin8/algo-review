Chapter 1 - Reliability, Scalable and Maintain Application 
Martin Kleppmann

- Todays apps are data intensive instead of compute intensive and not bc of the CPU but bc of the amount of data, the speed at which it is changing and the complexity of the data.
- A data intensive app is typically built from the standard building blocks that provide common needed functionality such as :
  	>store data so that they or another app can find it later aka databases
     	>remember the results of an expensive operation to speed up reads aka caches
	>allow users to search data by keyword or filter it in various ways aka search indexes 
	>send a message to another process to be handled asynchronously aka stream 				processing .
	>periodically crunch a large amount of accumulated data aka batch processing 

- Data systems are successful abstractions - we use them all the time without thinking too much. Reality is that it is not that simple because there are db with different characteristics bc different applications have different requirements. There are various approaches of caching, several ways to build search indexes and so on. When building an application we need to figure out which tools and which approaches are appropriate for the task at hand. It is also hard to combine tools when you need to do something that a single tool cannot do alone. 
- We typically think of databases, queues, caches and etc as being different categories of tools so why do they go under the umbrella term: data systems ? Some may have similarities such as storing data for some time but have very different access patterns which means different performance characteristics and thus very diff implementations. One reason is bc they no longer fit one traditional category anymore bc the categories are becoming blurred via Redis and Apache Kafka. Second, increasingly many applications now have demanding or wide-ranging requirements that a single tool can no longer meet all of its data processing and storage needs - we now use different tools and stitch them together. 
- Three concerns that are important to most software systems are - reliability, scalability and maintainability. 
- Reliability: the system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity (hardware/software faults or even human error).
- Scalability: as the system grows (in data volume volume, traffic volume or complexity), there should be reasonable ways of dealing with that growth. 
- Maintainability:  Over time, many different people will work on a system (engineers and operations, both maintaining current behavior and adapting the system to new use cases) and they should all be able to work on it productively. 

- Reliability: for something to be reliable or unreliable, typical expectations for software include - the application performs that function the user expected; it can tolerate the user making mistakes or using the software in unexpected ways; its performance is good enough for the required use case, under the unexpected workload and data volume; the system prevents any unauthorized access and abuse - basically working correctly even when things for wrong. The things that go wrong are called faults and can cope with them are fault-tolerant or resilient. A fault is not the same as a failure - a fault is usually defined as one component of the system deviating from its specs whereas a failure is when the system, as a whole stops providing the required service to the user. It is impossible to reduce the probability of a fault to zero; therefore it is usually best to deign fault-tolerance mechanism that prevents faults from causing failures. Counterintuitively: in such a fault-tolerant systems, it can make sense to increase the rate of faults by triggering them deliberatively. Many critical bugs are actually due to poor error handling ands by deliberately inducing faults, you ensure that the fault-tolerance machinery is continually exercised and tested, which can increase your confidence that faults will be handled correctly when they occur naturally. 
- Hardware faults: examples include Hard disks crash, RAM become faulty, the power grid has blackout or someone unplugs the wrong network cable. They have a mean time to failure of about 10 to 50 years, we can expect one disk to die a day in a cluster of 10,000. The first response is usually to add redundancy to the individual hardware components in order to reduce the failure rate of the system. Disks may be set up in a RAID configuration, servers may have dual power and etc. When one component dies, the redundant component takes its place while the broken component is replaced; this approach cannot completely prevent hardware problems but can keep a machine running for years uninterrupted. However, as data volumes and applications computing demands have increased, more applications have begun using larger number of machines which proportionally increases the rate of hardware faults. Like AWS, some platforms are designed to prioritize flexibility and elasticity over single-machine reliability. Hence, there is a move now towards system that can tolerate the loss of entire machines by using software fault-tolerance techniques in preference or in addition to hardware redundancy. Such systems have operational advantages - a single server system requires planning downtime if you need to reboot the machine (operating security patches) whereas a system that can tolerate machine failure can be patched one node at a time without downtime of an entire system. (Rolling upgrade?).
- Software faults: systematic errors within the system. Such faults are hard to anticipate and bc they are correlated across nodes they tend to cause more system failures than uncorrelated hardware faults. Examples include: a software bug that causes every instance of an application server to crash when given a particular bad input; a runaway process that uses up some shared resource like CPU time, memory, disk space or network bandwidth; a service that the system depends on that slows down, becomes unresponsive or starts returning corrupted responses; cascading failures, where a small fault in one component triggers a fault in another component, which in turns triggers further faults. The bugs that causes these kinds of software faults often lie dormant for a long time until they are triggered by an unusual set of circumstances, in those circumstances it is reveled that the software is making some kind of assumption about its environment - and while that assumption is usually true; it eventually stops being true for some reason. There is no quick solution to the systematic faults in a software but small things can help - carefully thinking about the assumptions and interaction s in the system; thorough testing; process insolation; allowing processes to crash and restart; measuring, monitoring, and analyzing system behavior in production. 
- Human errors: one study of a large internet services found that configuration errors by operators were the leading causes of outages. We make systems more reliable inspire of unreliable humans via designing systems in a way that minimizes opportunities for errors through well-designed abstractions, APIs and admin interfaces make it easy to the right thing and discourage the wrong thing; decouple the places where people make the most mistakes from the places where they can cause failures through providing a fully featured non-production sandbox environments where people can explore and experiment safely using real data without affecting real users; test thoroughly at all levels - from unit tests to whole-system integration tests and manual tests such as automated testing; allow quick and easy recovery from human errors, to minimizing the impact in the case of failure - for example make it fast to roll back configuration changes, roll out new code gradually so new bugs can affect only a small subset of users and provide tools to recompute data incase it turns out that the old computations was incorrect; set up detailed and clear monitoring such as performance metrics and error rates - telemetry - monitoring can show us early warning signs and allows us to check whether any assumptions or constraints are being violated, when a problem occurs metrics can be invaluable in diagnosing the issue; lastly implant good management practices and training.  
- How important is Reliability: bugs in business application cause lost productivity and legal risks if figures are reported incorrectly; outages in e-commerce sites can have huge costs in terms of lost revenue and damage to reputation; non-critical application we still have a responsibility to our users - consider a parent who stores all their pictures and videos of their children in their photo app and it suddenly becomes corrupted ? There are situations in which we may choose to sacrifice reliability in order to reduce development cost or operational cost but you should be very conscious when cutting such corners. 

- Scalability: even if a system is working reliable today doesn’t meant it will necessarily work reliable in the future and one common reason is degradation through increased load - 1 million concurrent users to 10 million - it is still processing much larger volumes of data than it did before. Scalability is the term we use to describe a system’s ability to cope with increased load. Scalability means considering questions like “if the system grows in a particular way, what are our options for coping with the growth?” And “how can we add computing resources to handle the additional load?”
- Describe Load: first we have to describe the current load of a system then can we discuss growth questions. Load can be described with a few numbers which we call load parameters. The best choice of parameters depends on the architecture of your system: it may be requests per second to a web server, the ratio of reads to write in a db, the hit rate on a cache. Perhaps the average case is what matters to you or maybe you prefer a bottleneck that is dominated by a small number of extreme cases. 
- Fanout: Page 11. 
- Twitter Example Page 11-13
- Describing Performance: once you have described the load on your system, you can investigate what happens when the load increases, you can look at it in 2 ways: when you increase a load parameter and keep the system resources (CPU, memory, network bandwidth, etc) unchanged, how is the performance of your system affected? And when you increase a load parameter, how much do you need to increase the resources if you want to keep performance unchanged? Both questions require performance numbers - in a batch processing system we usually care about he throughput - the number of records we process per second or the total time it takes to run a job on a dataset of a certain time. In online systems, what’s usually more important is the service response time - that is, the time between a client sending a request and receiving a response. 
- Latency and response time are often used synonymously, but they are not the same. The response time is what the client sees: besides the actual time to process the request aka the service time, it includes network delays and queuing delays. Latency is the duration that a request is waiting to be handled - during which it is latent, awaiting service. 
- Even if you make the same request over and over again, you’ll get slightly different response time on every try bc in practice, in a system handling a variety of requests, the response time can vary a lot. We therefore need to think of response time not as a single number, but as a distraction of values that you can measure. Most requests are reasonably fast but there are occasional outliers that take much longer and can vary in random additional latency could be introduced by a context switch to a background process, the loss of a network packet and TCP retransmission and many other causes - can also be intrinsically expensive bc they process more data.
- Its common to see the average response time of a service reported but the mean isn’t a very good metric if you want to know your typical response time bc it doesn’t tell you how many users actually experience the delay. Usually it is better to use percentiles - if you take your list of response time and sort it from fastest to slowest then the median is the halfway point - half the user requests are served less than the median response time ands the other half take more than half the response time if it is the 50th percentile aka p50. Also the median refers to a single request, more requests will increase the probability that at least one of them is slower than the median is much greater than 50%.
- In order to figure out how bad your outliers are, you can look at higher percentiles: the 95th, 99th and 99.9th percentiles are common. They are thresholds at which 95%, 99%, 99.9% of requests are faster than the particular threshold. For example is the 95th percentile response time is 1.5 seconds that means 95 out of 100 requests take less than 1.5 seconds and 5 out of 100 requests take 1.5 seconds or more. Higher percentiles of response times are known as tail latencies and are important bc they directly affect user’s experience of the service.
- Percentiles are often used in service level objectives and service level agreements (SLOs and SLAs), contract that define the expected performance and availability of a service. An SLA may state that a service is considered to be up if it has a median response time of less than 200 ms and a 99th percentile under 1s and the service will be up at least 99.9% of the time. These metrics set expectations for clients of the service and allow customers to demand a refund if the SLA is not met. 
- Queuing delays often account for a large part of the response time at high percentiles. As a server can only process a small number of things in parallel - it only takes a small number of slow requests to hold up the processing of subsequent requests - an effect sometimes known as head-of-line blocking. Even if those subsequent requests are fast to process on the servers the client will see a slow overall response time due to the time waiting for a prior request to complete. Due to this effect, it is important to measure response times on the client side. 
- Percentile in Practice
- Approached for Coping with Load: how do we maintain good performance even when our load parameters increase by the same amount? An architecture that is appropriate for one level of load is unlikely to cope with 10 times that load. If you are working on a fast growing service it is therefore likely that you will need to rethink your architecture on every order of magnitude load increases or perhaps more than that. Scaling up is vertical scaling - moving to a more powerful machine; scaling out - is horizontal scaling which is distributing load across multiple machines aka shared-nothing architecture. A system that can run on only one machine is often ampler but high end machines can become very expensive so very intensive workloads often cant avoid scaling out. A good architecture usually involves a pragmatic mixture of approaches: using several fairly powerful machines can be simpler and cheaper than a large number of small virtual machines. Some systems are elastic, meaning that they can automatically add computing resources hen they detect a load increase, whereas other systems are scaled manually - a human analyzes the capacity and decides to add more machines to the system. An elastic system can be useful if load is highly unpredictable, but manually scaled systems are simpler and may have fewer operational surprises.  
- While distributing stateless service across multiple machines is fairly straightforward, taking stateless data systems from a single node to a distributed setup can introduce a lot of additional complexity - for this reason it is conventional wisdom use to be that we keep our database on a single node (scale up) until scaling cost or higher availability requirements forced you to make it distributed. However, as the tools and abstractions for distributed systems get better, this common wisdom may change for at least some kinds of applications. 
- The architecture of systems that operate at large scale is usually highly specific to the application - there is no such thing as generic, one-size-fits-all scalable architecture aka magic scaling sauce. The problem can be the volume of reads, the volume of writes, the volume of data to store, the complexity of the data and the response time requirements and etc. 
- An architecture that scales well for a particular application is built around assumptions of which operations will be common and which will be rare - the load parameters. If those assumptions end up to be wrong, the engineering effort for scaling is at best wasted and at worst counterproductive. In an early stage startup or an unproven product its usually more important to be able to iterate quickly on product features than it is to scale hypothetical future load. 

- Maintainability: it is well known the majority of the cost of software is not in its initial development but in its ongoing maintenance - fixing b bugs, keeping its systems operational, investigating failures, adapting it to new platforms, modifying it for new use cases, repaying technical debt and adding new features. 
- People working on software systems dislike maintenance of legacy systems p it involves fixing other peoples mistakes, or working with platforms that are now outdated, or systems that were forced to do things they never intended for. 
- We should design software in such a way that it will hopefully minimize pain during maintenance and this avoid creating legacy software ourselves. We will pay mind to three design principles for software systems: operability - make it east for operations team to keep the system running smoothly; simplicity - make it easy for new engineers to understand the system by removing as much complexity as possible from the system; evolvability - make it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change. Also known as extensibility , modifiability or plasticity. 
- Operability: Operation teams are vital to keeping a software system running smoothly, a good team typically is responsible for the following: monitoring the health of the system and quickly restoring service if it goes into a bad state; tracking down the cause of problems, such as system failures or degraded performance; keeping software and platforms up to date, including security patches; keeping tabs on how different systems affect each others so that a problematic change can be avoided before it causes damage; anticipating future problems and solving them before they occur like capacity planning; establishing good practices and tools for deployment, configuration management and more; performing complex maintenance tasks, such as moving an application from one platform to another; maintaining the security of the system as configuration changes are made; defining processes that make operations predictable; help keep the production environment stable and preserving the organizations knowledge about he system even as individual people come and go. 
- Data systems can do various things to make routine tasks easy, including: providing visibility into the runtime behavior and internals of the system, with good monitoring; providing a good support for automation and integration with standard tools; avoiding dependency on individuals machines (allowing machines to be taken down for maintenance while the system as whole continues running uninterrupted); providing good default behavior, but also giving administrators the freedom to override defaults when needed; providing good documentation to an easy-to-understand operational model; self-healing where appropriate but also giving administrators manual control over the system state when needed; exhibiting predictable behavior, minimizing surprises. 
- Simplicity: a software project mired in complexity is sometimes described as a big ball of mud. There are various possible symptoms of complexity: explosion of state space, tight coupling of modules, tangled dependencies, inconsistent naming and terminology, hacks aimed at solving performance problems, special casing to work around issues elsewhere and many more. When complexity makes maintenance hard, budgets and schedules are often overrun. In complex software, there is also a greater risk fo introducing bugs when making a change: when a system is harder for developers to understand and reason about hidden assumptions, unintended consequences and unexpected interactions are more easily overlooked. Conversely, reducing complexity greatly improves the maintainability of software and thus simplicity should be a key goal for the systems we build. Removing accidental complexity - the complexity as accidental if it is not inherent in the problem that the software solves but arises only from implementation.
- One of the best tools we have for removing accidental complexity is abstraction. A good abstraction can hide a great deal of implementation detail behind a clean simple-to-understand cascade. A good abstraction can be also used for more a wide range of different applications. Not only is this refuse more efficient than re-implementing a similar thing multiple times but it also leads to higher-quality software as quality improvements in the abstracted component benefit all applications that use it. 
- Example: high level programming languages are abstractions that huge machine code, CPU registers and sys calls. SQL is an abstraction that hides complex on-disk and in-memory data structures, concurrent requests from other clients and inconsistencies after crashes. When programming in a high-level language, we are still using machine code; we are just not using it directly because the programming language abstraction save us from having to think about it. 
- Finding a good abstraction is very hard - in the field os distributed systems although there are many good algorithms, it is much less clear how we should be packaging them into abstractions that help us keep the complexity of the system at manageable level. 
- Evolvability: your system will be changed. They are much more likely to be in a constant flux; you learn new facts, previously unanticipated use cases emerge, business priorities change, user request new features, new platforms replace old platforms, legal or regulatory requirements change, growth of the system forces architectural changes and etc. 
- In terms of organizational process - AGILE working patterns provide a framework for adapting to change. The agile community has also developed technical tools and patterns that are helpful when developing software in frequently changing environment, such as test-driven development and refactoring. 
